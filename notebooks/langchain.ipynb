{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8d9bf7",
   "metadata": {},
   "source": [
    "# ðŸ”— LangChain + LLM Abstractions\n",
    "\n",
    "In this notebook weâ€™ll move from the raw OpenAI SDK to **LangChain**, which provides higher-level abstractions to make LLM use easier.\n",
    "\n",
    "Weâ€™ll cover:\n",
    "1. Setting up `ChatOpenAI`.  \n",
    "2. Prompt templates & the **LangChain Expression Language (LCEL)**.  \n",
    "3. Varying parameters like `temperature` and `top_p`.  \n",
    "4. **Streaming** responses with callbacks.  \n",
    "5. **Batching & parallel calls** with `.batch()`.  \n",
    "6. Getting **structured outputs** with Pydantic models.  \n",
    "7. Swapping configs at runtime (`.bind`, `.with_config`).  \n",
    "8. (Optional) Using Azure OpenAI through LangChain.\n",
    "\n",
    "By the end, youâ€™ll see how LangChain **simplifies interaction, chaining, and integration** with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c722d761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Here are two quick tips for learning Python:\\n\\n1. **Practice Regularly**: Consistency is key when learning a programming language. Set aside time each day or week to practice coding. Work on small projects, solve coding challenges on platforms like LeetCode or HackerRank, and try to implement what you learn in real-world scenarios.\\n\\n2. **Utilize Online Resources**: Take advantage of the wealth of online resources available. Websites like Codecademy, freeCodeCamp, and Coursera offer interactive Python courses. Additionally, the official Python documentation is a great reference for understanding language features and libraries. Engaging with community forums like Stack Overflow can also help you troubleshoot and learn from others' experiences.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 16, 'total_tokens': 160, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CKP5f84GTSaON3xkz1Og7QdyXojai', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--63745959-439a-4542-92b5-69ad828a0279-0', usage_metadata={'input_tokens': 16, 'output_tokens': 144, 'total_tokens': 160, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,   # same knobs as OpenAI\n",
    "    # top_p=0.9,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "llm.invoke(\"Give me two quick tips for learning Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a230f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, concise assistant.\"),\n",
    "    (\"user\", \"Summarize this in 2 bullets:\\n\\n{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d77e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Transformers use attention to weigh context; embeddings turn tokens into vectors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful, concise assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Summarize this in 2 bullets:\\n\\nTransformers use attention to weigh context; embeddings turn tokens into vectors.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_rendered = prompt.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4524de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt_rendered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb98571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Transformers utilize attention mechanisms to prioritize context in processing information.\n",
      "- Embeddings convert tokens into vector representations for effective data handling.\n"
     ]
    }
   ],
   "source": [
    "content = StrOutputParser().invoke(response)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d191f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentle drops descend,  \n",
      "Whispers weave through emerald,  \n",
      "Earth drinks in the sound.  \n",
      "Clouds embrace the twilight,  \n",
      "Nature's pulse in rhythm.  \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class PrintHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        print(token, end=\"\")\n",
    "\n",
    "stream_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, streaming=True, callbacks=[PrintHandler()])\n",
    "(stream_llm | StrOutputParser()).invoke(\"Stream a 5-sentence haiku about rain.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b14a61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is overfitting?\n",
      "A: Overfitting is a common problem in machine learning and statistical modeling where a model learns not only the underlying patterns in the training data but also the noise and outliers. This results in a model that performs very well on the training data but poorly on unseen data or test data. Essentially, the model becomes too complex and tailored to the specific dataset it was trained on, losing its ability to generalize to new, unseen examples.\n",
      "\n",
      "Key characteristics of overfitting include:\n",
      "\n",
      "1. **High Training Accuracy**: The model achieves very high accuracy on the training dataset.\n",
      "2. **Low Test Accuracy**: The model performs poorly on validation or test datasets, indicating that it cannot generalize well.\n",
      "3. **Complexity**: Overfitting often occurs with overly complex models that have too many parameters relative to the amount of training data.\n",
      "\n",
      "To mitigate overfitting, several strategies can be employed:\n",
      "\n",
      "- **Simplifying the Model**: Using a less complex model that has fewer parameters.\n",
      "- **Regularization**: Techniques like L1 (Lasso) or L2 (Ridge) regularization can help penalize overly complex models.\n",
      "- **Cross-Validation**: Using techniques like k-fold cross-validation to ensure that the model's performance is consistent across different subsets of the data.\n",
      "- **Pruning**: In decision trees, for example, pruning can help reduce complexity.\n",
      "- **Early Stopping**: Monitoring the model's performance on a validation set and stopping training when performance starts to degrade.\n",
      "- **Data Augmentation**: Increasing the size of the training dataset by creating modified versions of the data.\n",
      "\n",
      "By employing these techniques, one can help ensure that a model generalizes better to new data, thereby reducing the risk of overfitting.\n",
      "\n",
      "Q: Explain dropout in one line.\n",
      "A: Dropout is a regularization technique in neural networks that randomly sets a fraction of the neurons to zero during training to prevent overfitting.\n",
      "\n",
      "Q: Contrast precision vs recall briefly.\n",
      "A: Precision and recall are two important metrics used to evaluate the performance of classification models, particularly in scenarios where the classes are imbalanced.\n",
      "\n",
      "- **Precision** measures the accuracy of the positive predictions made by the model. It is calculated as the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). High precision indicates that when the model predicts a positive class, it is likely to be correct.\n",
      "\n",
      "  \\[\n",
      "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
      "  \\]\n",
      "\n",
      "- **Recall** (also known as sensitivity or true positive rate) measures the model's ability to identify all relevant instances. It is calculated as the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). High recall indicates that the model captures most of the positive cases.\n",
      "\n",
      "  \\[\n",
      "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
      "  \\]\n",
      "\n",
      "In summary, precision focuses on the quality of positive predictions, while recall emphasizes the model's ability to find all positive instances. Balancing both metrics is often necessary, especially in applications where false positives and false negatives have different costs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is overfitting?\",\n",
    "    \"Explain dropout in one line.\",\n",
    "    \"Contrast precision vs recall briefly.\"\n",
    "]\n",
    "# Runnable.batch for parallel execution\n",
    "answers = (llm | StrOutputParser()).batch(questions)\n",
    "for q, a in zip(questions, answers):\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23501a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flashcard(term='Positional Encoding', definition='A technique used in Transformers to inject information about the position of tokens in a sequence, enabling the model to understand the order of words.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Flashcard(BaseModel):\n",
    "    term: str = Field(..., description=\"Short term\")\n",
    "    definition: str = Field(..., description=\"One-sentence definition\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Flashcard)  # Let LC coax JSONâ†’model\n",
    "card = structured_llm.invoke(\"Create a flashcard about 'positional encoding' in Transformers.\")\n",
    "card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a4b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
